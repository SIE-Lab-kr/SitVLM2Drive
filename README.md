# SitLLM2Drive

**SitLLM2Drive** is a dataset for autonomous vehicle scene understanding, featuring:
- 10,000+ richly annotated frames
- Object-level causal graphs
- Scene-level planning and safety metadata
- Over 2 million QA pairs for driving scenarios

## ğŸ“ Dataset Structure
dataset/

â”œâ”€â”€ video_0001.json # scene annotations

frames/

â”œâ”€â”€ video_0001/ # corresponding images

## âœï¸ Annotations
- **Caption**: Free-text scene summary
- **Maneuver**: Vehicle's goal (e.g. "Turn left")
- **Graph**: Causal object relationships

See `annotations_format.md` for full schema.


## ğŸ“œ License
[MIT License](LICENSE)

## ğŸ”§ Scripts
Use tools in `scripts/` for analysis and statistics extraction.

## ğŸ“Š Statistics
- 495 videos
- 10k frames
- 2.08M Q&A pairs
- 76 unique object types

## ğŸ’¡ Applications
- Causal reasoning in traffic
- Visual question answering for AVs


**Tasks**: Perception, Planning, Action Suggestions, Planning under uncertainty, Causal Reasoning, etc

**Modality**: Vision + Text + Graphs  

**Safety-Critical**: 24% of scenarios  
