{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f493296",
   "metadata": {},
   "source": [
    "You are an expert evaluator of autonomous driving reasoning.\n",
    "\n",
    "You will be given:\n",
    "- A driving scenario description.\n",
    "- A question about the scenario.\n",
    "- A candidate answer produced by a model.\n",
    "\n",
    "Your job is to evaluate how strong the answer's **causal reasoning** is: how clearly, correctly, and completely it explains *why* things happen, *how* objects and events influence each other, and *why* the autonomous vehicle would choose certain actions.\n",
    "\n",
    "Scoring guidelines (1–10):\n",
    "\n",
    "1–3: Very poor causality\n",
    "- Little or no cause-effect explanation.\n",
    "- Mostly restates the question or scenario without explaining “why”.\n",
    "- Important causal factors are missing or wrong.\n",
    "\n",
    "4–6: Weak / partial causality\n",
    "- Some causal links, but shallow or incomplete.\n",
    "- Misses several important factors or mixes up causes and effects.\n",
    "- Reasoning may be partially correct but not coherent.\n",
    "\n",
    "7–8: Good causality\n",
    "- Mostly correct, coherent chain of cause-effect.\n",
    "- Identifies key objects, risks, and how they influence AV actions.\n",
    "- Minor omissions or small mistakes, but overall logically sound.\n",
    "\n",
    "9–10: Excellent causality\n",
    "- Clear, detailed, and technically accurate chain of cause-effect.\n",
    "- Correctly explains how the AV perceives, predicts, and decides actions.\n",
    "- Explicitly links objects, risks, and traffic rules to the AV’s decisions.\n",
    "\n",
    "Return your evaluation ONLY as a JSON object in this exact format:\n",
    "\n",
    "{\n",
    "  \"causality_score\": <integer from 1 to 10>,\n",
    "}\n",
    "\n",
    "Now here is the data:\n",
    "\n",
    "SCENARIO:\n",
    "{scenario_text}\n",
    "\n",
    "QUESTION:\n",
    "{question_text}\n",
    "\n",
    "MODEL_ANSWER:\n",
    "{model_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1ebb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # use env var, not hard-coded\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o-mini\"  # or \"gpt-4o\"\n",
    "\n",
    "def build_scenario_text(caption, speed, steering, objects_info, relations_info):\n",
    "    # Reuse your extracted info to form a compact text block for the judge\n",
    "    objects_lines = []\n",
    "    for obj in objects_info:\n",
    "        objects_lines.append(\n",
    "            f\"- {obj['name']} (type: {obj['type']}, \"\n",
    "            f\"status: {', '.join(obj['status']) if obj['status'] else 'N/A'}, \"\n",
    "            f\"safety: {', '.join(obj['safety']) if obj['safety'] else 'N/A'}, \"\n",
    "            f\"positions: {', '.join(obj['positions']) if obj['positions'] else 'N/A'}, \"\n",
    "            f\"importance: {obj['importance']})\"\n",
    "        )\n",
    "    objects_text = \"\\n\".join(objects_lines) if objects_lines else \"No objects described.\"\n",
    "\n",
    "    if relations_info:\n",
    "        relations_text = \"\\n\".join(\n",
    "            [f\"- {src} --> {tgt}: {rel}\" for (src, tgt, rel) in relations_info]\n",
    "        )\n",
    "    else:\n",
    "        relations_text = \"No explicit relations found.\"\n",
    "\n",
    "    scenario_text = (\n",
    "        f\"Caption: {caption}\\n\"\n",
    "        f\"Speed: {speed}\\n\"\n",
    "        f\"Steering: {steering}\\n\\n\"\n",
    "        f\"Objects:\\n{objects_text}\\n\\n\"\n",
    "        f\"Relations:\\n{relations_text}\"\n",
    "    )\n",
    "    return scenario_text\n",
    "\n",
    "\n",
    "def build_causality_prompt(scenario_text, question_text, model_answer):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert evaluator of autonomous driving reasoning.\n",
    "\n",
    "You will be given:\n",
    "- A driving scenario description.\n",
    "- A question about the scenario.\n",
    "- A candidate answer produced by a model.\n",
    "\n",
    "Your job is to evaluate how strong the answer's **causal reasoning** is: how clearly, correctly, and completely it explains *why* things happen, *how* objects and events influence each other, and *why* the autonomous vehicle would choose certain actions.\n",
    "\n",
    "Scoring guidelines (1–10):\n",
    "\n",
    "1–3: Very poor causality\n",
    "- Little or no cause-effect explanation.\n",
    "- Mostly restates the question or scenario without explaining “why”.\n",
    "- Important causal factors are missing or wrong.\n",
    "\n",
    "4–6: Weak / partial causality\n",
    "- Some causal links, but shallow or incomplete.\n",
    "- Misses several important factors or mixes up causes and effects.\n",
    "- Reasoning may be partially correct but not coherent.\n",
    "\n",
    "7–8: Good causality\n",
    "- Mostly correct, coherent chain of cause-effect.\n",
    "- Identifies key objects, risks, and how they influence AV actions.\n",
    "- Minor omissions or small mistakes, but overall logically sound.\n",
    "\n",
    "9–10: Excellent causality\n",
    "- Clear, detailed, and technically accurate chain of cause-effect.\n",
    "- Correctly explains how the AV perceives, predicts, and decides actions.\n",
    "- Explicitly links objects, risks, and traffic rules to the AV’s decisions.\n",
    "\n",
    "Return your evaluation ONLY as a JSON object in this exact format:\n",
    "\n",
    "{{\n",
    "  \"causality_score\": <integer from 1 to 10>\n",
    "}}\n",
    "\n",
    "Now here is the data:\n",
    "\n",
    "SCENARIO:\n",
    "{scenario_text}\n",
    "\n",
    "QUESTION:\n",
    "{question_text}\n",
    "\n",
    "MODEL_ANSWER:\n",
    "{model_answer}\n",
    "\"\"\".strip()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def score_causality(caption, speed, steering, objects_info, relations_info,\n",
    "                    question_text, model_answer,\n",
    "                    model=EVAL_MODEL, temperature=0.0, max_tokens=300):\n",
    "    \"\"\"\n",
    "    Returns a dict: {\"causality_score\": int, \"justification\": str}\n",
    "    \"\"\"\n",
    "    scenario_text = build_scenario_text(caption, speed, steering, objects_info, relations_info)\n",
    "    prompt = build_causality_prompt(scenario_text, question_text, model_answer)\n",
    "\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            data = json.loads(content)\n",
    "            return {\n",
    "                \"causality_score\": int(data.get(\"causality_score\", 0))\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            # If response isn't valid JSON, you can either retry or return a default\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return {\"causality_score\": 0}\n",
    "        except openai.error.OpenAIError:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            return {\"causality_score\": 0, \"justification\": \"API error during evaluation.\"}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
